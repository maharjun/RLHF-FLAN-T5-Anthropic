defaults:
  - /simman_config
  - model: attention_pooled_encoder

# Simmanager Parameter overrides
sim_name: Reward-Model-Training

# Core Parameters
seed: 103
device_name: ${gpu_if_available:0}
use_multi_gpu_if_available: True

# Data parameters
data:
  seed: 192
  name: Anthropic/hh-rlhf
  subdirs: [harmless-base, helpful-base, helpful-online, helpful-rejection-sampled]
  val_fraction: 0.1  # val as a fraction of train data
  max_tokens: 512
  pretrain_batch_size_per_device: 128

# Model Parameters
model:
  transformer_name: google/flan-t5-small
  train_transformer: False
  use_pretrained_output: True


# Training Parameters
training:
  optimizer:
    type: Adam  # Adam / AdamW
    init_lr: 1.0e-5
    final_lr: 2.0e-6
    weight_decay: 0.0
    correct_bias: False

  loop:
    max_non_optim_vals: 10
    train_batch_size: 512
    val_batch_size: 512
    n_val_data: 8196
    n_epochs: 10
    n_batches: 0

    intervals:
      validation:
        interval_val: 10
        interval_type: per-epoch
        max_n_intervals: -1
      train_status_update:
        interval_val: 24
        interval_type: per-epoch
        max_n_intervals: -1
      progress_logging:
        interval_val: 40
        interval_type: per-epoch
        max_n_intervals: -1
      checkpointing:
        interval_val: 10
        interval_type: per-epoch
        max_n_intervals: -1

loggers:
  training_loop:
    name: training_loop
    level: INFO
    to_stdout: True
  train_reward_model:
    name: train_reward_model
    level: INFO
    to_stdout: True
