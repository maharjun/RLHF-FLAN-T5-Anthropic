defaults:
  - /simman_config

# Simmanager Parameter overrides
sim_name: Reward-Model-Training

# Core Parameters
seed: 103
device_name: ${gpu_if_available:}

# Data parameters
data:
  seed: 192
  name: Anthropic/hh-rlhf
  subdir: helpful-rejection-sampled
  val_fraction: 0.1  # val as a fraction of train data

# Model Parameters
model:
  transformer_name: google/flan-t5-small
  attention_inner_dim: 256
  pooling_output_dim: 256
  readout_additional_layers: []

# Training Parameters
training:
  optimizer:
    type: AdamW  # Adam / AdamW
    lr: 0.0001
    weight_decay: 0.001

  loop:
    max_non_optim_vals: 20
    train_batch_size: 32
    val_batch_size: 32
    n_val_data: 64
    n_epochs: 0
    n_batches: 2

    intervals:
      validation:
        interval_val: 40
        interval_type: batches
        max_n_intervals: -1
      train_status_update:
        interval_val: 10
        interval_type: batches
        max_n_intervals: -1

loggers:
  training_loop:
    name: training_loop
    level: INFO
    to_stdout: True
  train_reward_model:
    name: train_reward_model
    level: INFO
    to_stdout: True
